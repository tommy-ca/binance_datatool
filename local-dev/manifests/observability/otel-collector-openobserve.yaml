---
# OpenTelemetry Collector Configuration for OpenObserve Integration
# Unified telemetry collection for Crypto Lakehouse
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-openobserve-config
  namespace: observability
data:
  config.yaml: |
    receivers:
      # OTLP receiver for applications
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      
      # Prometheus receiver for scraping metrics
      prometheus/internal:
        config:
          scrape_configs:
          - job_name: 'otel-collector'
            scrape_interval: 15s
            static_configs:
            - targets: ['localhost:8888']
          
          # Scrape OpenObserve metrics
          - job_name: 'openobserve'
            scrape_interval: 30s
            static_configs:
            - targets: ['openobserve.observability:5080']
            metrics_path: '/metrics'
            
          # Prefect Server metrics
          - job_name: 'prefect-server'
            scrape_interval: 30s
            static_configs:
            - targets: ['prefect-server.prefect:4200']
            metrics_path: '/api/metrics'
            
          # MinIO metrics
          - job_name: 'minio'
            scrape_interval: 30s
            static_configs:
            - targets: ['minio-service.minio:9000']
            metrics_path: '/minio/v2/metrics/cluster'
            
          # s5cmd executor metrics
          - job_name: 's5cmd-executor'
            scrape_interval: 15s
            static_configs:
            - targets: ['s5cmd-executor.s5cmd:8080']
            metrics_path: '/metrics'

      # Host metrics for system monitoring
      hostmetrics:
        collection_interval: 30s
        scrapers:
          cpu:
            metrics:
              system.cpu.utilization:
                enabled: true
          memory:
            metrics:
              system.memory.utilization:
                enabled: true
          disk:
            metrics:
              system.disk.io:
                enabled: true
              system.disk.operations:
                enabled: true
              system.disk.io_time:
                enabled: true
          network:
            metrics:
              system.network.io:
                enabled: true
              system.network.packets:
                enabled: true
          filesystem:
            metrics:
              system.filesystem.utilization:
                enabled: true
          load:
            metrics:
              system.cpu.load_average.1m:
                enabled: true
              system.cpu.load_average.5m:
                enabled: true
              system.cpu.load_average.15m:
                enabled: true

      # Kubernetes receiver for cluster metrics
      k8s_cluster:
        auth_type: serviceAccount
        node_conditions_to_report: [Ready, MemoryPressure, DiskPressure, PIDPressure]
        allocatable_types_to_report: [cpu, memory, storage]
        
      # Kubernetes events receiver
      k8s_events:
        auth_type: serviceAccount

    processors:
      # Batch processor for efficiency
      batch:
        timeout: 1s
        send_batch_size: 1024
        send_batch_max_size: 2048
        
      # Memory limiter to prevent OOM
      memory_limiter:
        limit_mib: 1024
        spike_limit_mib: 256
        check_interval: 5s
        
      # Resource processor to add metadata
      resource:
        attributes:
        - key: environment
          value: local
          action: upsert
        - key: cluster
          value: crypto-lakehouse-local
          action: upsert
        - key: version
          value: "1.0.0"
          action: upsert
        - key: datacenter
          value: local-dev
          action: upsert

      # Add service name for traces without one
      resource/traces:
        attributes:
        - key: service.name
          value: "unknown-service"
          action: insert

      # K8s attributes processor
      k8sattributes:
        auth_type: serviceAccount
        passthrough: false
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection

      # Transform processor for custom metrics
      transform:
        metric_statements:
        - context: metric
          statements:
          - set(description, "Crypto Lakehouse metrics") where name == "s5cmd_operations_total"
          - set(unit, "operations") where name == "s5cmd_operations_total"

    exporters:
      # OpenObserve exporter for unified observability
      otlp/openobserve:
        endpoint: http://openobserve.observability:4317
        tls:
          insecure: true
        headers:
          "organization": "crypto-lakehouse"
          "stream-name": "crypto-lakehouse-telemetry"
        
      # HTTP exporter for OpenObserve logs/traces
      otlphttp/openobserve:
        endpoint: http://openobserve.observability:4318
        tls:
          insecure: true
        headers:
          "organization": "crypto-lakehouse"
          "stream-name": "crypto-lakehouse-telemetry"
          "Content-Type": "application/x-protobuf"
        
      # Debug exporter for troubleshooting
      debug:
        verbosity: detailed
        sampling_initial: 5
        sampling_thereafter: 200
        
      # File exporter for backup/debugging
      file:
        path: /tmp/otel-data.json
        rotation:
          max_megabytes: 100
          max_days: 7
          max_backups: 3

    extensions:
      # Health check extension
      health_check:
        endpoint: 0.0.0.0:13133
        
      # Performance profiling
      pprof:
        endpoint: 0.0.0.0:1777
        
      # zPages for debugging
      zpages:
        endpoint: 0.0.0.0:55679
        
      # Memory ballast for stability
      memory_ballast:
        size_mib: 256

    service:
      extensions: [health_check, pprof, zpages, memory_ballast]
      
      pipelines:
        # Traces pipeline
        traces:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, resource/traces, batch]
          exporters: [otlp/openobserve, debug]
          
        # Metrics pipeline
        metrics:
          receivers: [otlp, prometheus/internal, hostmetrics, k8s_cluster]
          processors: [memory_limiter, k8sattributes, resource, transform, batch]
          exporters: [otlp/openobserve, debug]
          
        # Logs pipeline
        logs:
          receivers: [otlp, k8s_events]
          processors: [memory_limiter, k8sattributes, resource, batch]
          exporters: [otlphttp/openobserve, debug]
          
      telemetry:
        logs:
          level: "info"
          development: false
          sampling:
            enabled: true
            tick: 10s
            initial: 5
            thereafter: 200
        metrics:
          address: 0.0.0.0:8888
          level: detailed
        traces:
          processors:
          - batch

---
# OpenTelemetry Collector Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
    component: telemetry-collector
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
        component: telemetry-collector
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8888"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: otel-collector
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.91.0
        args:
        - --config=/etc/otel-collector-config/config.yaml
        ports:
        - containerPort: 4317   # OTLP gRPC
          name: otlp-grpc
        - containerPort: 4318   # OTLP HTTP
          name: otlp-http
        - containerPort: 8888   # Prometheus metrics
          name: metrics
        - containerPort: 13133  # Health check
          name: health
        - containerPort: 1777   # pprof
          name: pprof
        - containerPort: 55679  # zpages
          name: zpages
        env:
        - name: GOGC
          value: "80"
        - name: GOMEMLIMIT
          value: "1024MiB"
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        volumeMounts:
        - name: config-volume
          mountPath: /etc/otel-collector-config
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        livenessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
      volumes:
      - name: config-volume
        configMap:
          name: otel-collector-openobserve-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      restartPolicy: Always

---
# Service for OpenTelemetry Collector
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
    component: telemetry-collector
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8888"
    prometheus.io/path: "/metrics"
spec:
  selector:
    app: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
    protocol: TCP
  - name: otlp-http
    port: 4318
    targetPort: 4318
    protocol: TCP
  - name: metrics
    port: 8888
    targetPort: 8888
    protocol: TCP
  - name: health
    port: 13133
    targetPort: 13133
    protocol: TCP
  - name: zpages
    port: 55679
    targetPort: 55679
    protocol: TCP
  type: ClusterIP

---
# ServiceAccount for OpenTelemetry Collector
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: observability

---
# ClusterRole for OpenTelemetry Collector
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - nodes/metrics
  - services
  - endpoints
  - pods
  - events
  - namespaces
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources:
  - deployments
  - replicasets
  - daemonsets
  - statefulsets
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources:
  - jobs
  - cronjobs
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics", "/metrics/cadvisor"]
  verbs: ["get"]

---
# ClusterRoleBinding for OpenTelemetry Collector
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector
subjects:
- kind: ServiceAccount
  name: otel-collector
  namespace: observability