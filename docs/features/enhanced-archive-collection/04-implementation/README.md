# Workflows Documentation

This directory contains comprehensive documentation for all Crypto Lakehouse workflows, focusing on data collection, processing, and orchestration patterns.

## ðŸ“‹ Available Workflows

### ðŸ”¥ Enhanced Archive Collection Workflow (v2.0)

**Status**: Production Ready âœ…  
**Last Updated**: July 19, 2025

The Enhanced Archive Collection Workflow provides comprehensive access to all Binance public archive data across all markets with optimized s5cmd performance.

#### Documentation Files

| Document | Description | Audience |
|----------|-------------|----------|
| **[Enhanced Archive Collection](./enhanced-archive-collection.md)** | Complete workflow guide with features, configuration, and usage | All Users |
| **[API Reference](./archive-collection-api.md)** | Detailed API documentation for developers | Developers |
| **[Examples & Use Cases](./archive-collection-examples.md)** | Practical examples and configuration patterns | All Users |
| **[Legacy Equivalents](./legacy-equivalents.md)** | Migration guide from legacy workflows | Migration Users |

#### Quick Overview

**Markets Supported:**
- âœ… **Spot** (`spot/`) - 5 data types + intervals
- âœ… **Futures UM** (`futures/um/`) - 10 data types + intervals
- âœ… **Futures CM** (`futures/cm/`) - 9 data types + intervals
- âœ… **Options** (`option/`) - 2 data types + intervals

**Key Features:**
- **28 Data Type Combinations** across all markets
- **Batch Processing** - Up to 100 files per batch
- **Parallel Downloads** - 8 concurrent streams
- **Resume Capability** - Skip existing files
- **Type-Safe Configuration** - Pydantic-based validation
- **Prefect Orchestration** - Enhanced observability

**Performance Metrics:**
- 10-50 MB/s throughput
- 3,134+ potential file combinations per day
- <2% failure rate with retry mechanisms
- 100% core functionality test coverage

## ðŸš€ Quick Start

### Basic Usage
```python
from src.crypto_lakehouse.workflows.archive_collection_prefect import PrefectArchiveCollectionWorkflow
from src.crypto_lakehouse.core.config import WorkflowConfig

config = WorkflowConfig({
    "workflow_type": "archive_collection",
    "matrix_path": "enhanced_binance_archive_matrix.json", 
    "output_directory": "output/data",
    "markets": ["spot", "futures_um"],
    "symbols": ["BTCUSDT", "ETHUSDT"],
    "data_types": ["klines", "trades"],
    "intervals": {"klines": ["1h", "1d"]},
    "date_range": {"start": "2025-07-15", "end": "2025-07-15"}
})

workflow = PrefectArchiveCollectionWorkflow(config)
result = await workflow.execute()
```

### Multi-Market Collection
```python
config = WorkflowConfig({
    "markets": ["spot", "futures_um", "futures_cm", "options"],
    "symbols": {
        "spot": ["BTCUSDT", "ETHUSDT"],
        "futures_um": ["BTCUSDT", "ETHUSDT"],
        "futures_cm": ["BTCUSD_PERP"],
        "options": ["BTC"]
    },
    "data_types": ["klines", "trades", "fundingRate", "BVOLIndex"],
    "batch_size": 100,
    "max_parallel_downloads": 8
})
```

## ðŸ“Š Data Types by Market

### Spot Market (5 data types)
- `klines` - Candlestick data (16 intervals)
- `trades` - Individual trade records
- `aggTrades` - Aggregated trade data
- `bookDepth` - Order book snapshots (6 intervals)
- `bookTicker` - Best bid/ask data

### Futures UM (10 data types)
- `klines` - Candlestick data (16 intervals)
- `trades` - Individual trade records
- `aggTrades` - Aggregated trade data
- `fundingRate` - Funding rate data
- `premiumIndex` - Premium index data
- `metrics` - Open interest, long-short ratios
- `indexPriceKlines` - Index price klines (16 intervals)
- `markPriceKlines` - Mark price klines (16 intervals)
- `bookDepth` - Order book snapshots (6 intervals)
- `bookTicker` - Best bid/ask data
- `liquidationSnapshot` - Liquidation data

### Futures CM (9 data types)
- `klines` - Candlestick data (16 intervals)
- `trades` - Individual trade records
- `aggTrades` - Aggregated trade data
- `fundingRate` - Funding rate data
- `premiumIndex` - Premium index data
- `indexPriceKlines` - Index price klines (16 intervals)
- `markPriceKlines` - Mark price klines (16 intervals)
- `bookDepth` - Order book snapshots (6 intervals)
- `bookTicker` - Best bid/ask data
- `liquidationSnapshot` - Liquidation data

### Options (2 data types)
- `BVOLIndex` - Binance Volatility Index (4 intervals)
- `EOHSummary` - End of hour summary

## âš¡ Performance Configurations

### High-Performance Setup
```json
{
    "batch_size": 100,
    "max_parallel_downloads": 8,
    "enable_batch_mode": true,
    "s5cmd_extra_args": ["--no-sign-request", "--retry-count=3", "--numworkers=8"]
}
```

### Memory-Optimized Setup
```json
{
    "batch_size": 20,
    "max_parallel_downloads": 2,
    "enable_batch_mode": false,
    "part_size_mb": 10
}
```

### Network-Resilient Setup
```json
{
    "enable_resume": true,
    "download_checksum": true,
    "timeout_seconds": 900,
    "s5cmd_extra_args": ["--retry-count=5"]
}
```

## ðŸ”§ Configuration Templates

### Research Dataset
```python
research_config = {
    "markets": ["spot", "futures_um", "futures_cm", "options"],
    "symbols": {
        "spot": ["BTCUSDT", "ETHUSDT", "ADAUSDT"],
        "futures_um": ["BTCUSDT", "ETHUSDT", "ADAUSDT"],
        "futures_cm": ["BTCUSD_PERP", "ETHUSD_PERP"],
        "options": ["BTC", "ETH"]
    },
    "data_types": [
        "klines", "trades", "fundingRate", "liquidationSnapshot",
        "BVOLIndex", "metrics", "bookDepth"
    ],
    "intervals": {
        "klines": ["1m", "5m", "1h", "1d"],
        "BVOLIndex": ["1h", "1d"],
        "bookDepth": ["1000ms"]
    }
}
```

### Trading System Backtesting
```python
backtest_config = {
    "markets": ["spot", "futures_um"],
    "symbols": ["BTCUSDT", "ETHUSDT"],
    "data_types": ["klines", "trades", "fundingRate", "liquidationSnapshot"],
    "intervals": {
        "klines": ["1m", "5m", "1h"]
    },
    "date_range": {
        "start": "2025-06-01",
        "end": "2025-07-15"
    }
}
```

### Real-time Pipeline Initialization
```python
pipeline_config = {
    "markets": ["spot", "futures_um"],
    "symbols": ["BTCUSDT", "ETHUSDT", "BNBUSDT"],
    "data_types": ["klines", "trades", "fundingRate", "bookTicker"],
    "intervals": {
        "klines": ["1m", "5m", "1h"]
    },
    "date_range": {
        "start": "2025-07-14",
        "end": "2025-07-15"
    },
    "force_redownload": True
}
```

## ðŸŽ¯ Use Case Examples

### Market Research
- **Data**: All markets, multiple symbols, comprehensive data types
- **Timeframe**: Weekly to monthly collections
- **Focus**: Pattern analysis, correlation studies, volatility research

### Algorithmic Trading
- **Data**: Specific markets, high-frequency intervals, execution data
- **Timeframe**: Daily collections with historical context
- **Focus**: Signal generation, execution analysis, risk management

### Risk Management
- **Data**: Futures markets, liquidation data, funding rates
- **Timeframe**: Continuous collection with real-time updates
- **Focus**: Position sizing, exposure analysis, stress testing

### Quantitative Analysis
- **Data**: Multi-market, long timeframes, statistical completeness
- **Timeframe**: Monthly to yearly historical collections
- **Focus**: Factor modeling, regime detection, portfolio optimization

## ðŸ§ª Testing and Validation

### Core Functionality Tests
```bash
# Test core logic
python test_core_functionality.py

# Test workflow components
python test_enhanced_workflow_simple.py

# Full integration tests
python -m pytest tests/ -v
```

### Performance Testing
```python
# Performance benchmark
config = {
    "batch_size": 100,
    "max_parallel_downloads": 8,
    "enable_batch_mode": True
}

# Expected: >1 file/second, >95% success rate
```

### Data Validation
```python
# Verify data completeness
result = await workflow.execute()
success_rate = result['success_rate']
assert success_rate > 95.0

# Check file integrity
total_size = result['collection_stats']['total_size_bytes']
assert total_size > 0
```

## ðŸ”„ Integration Patterns

### Prefect Orchestration
```python
from prefect import flow
from prefect.deployments import Deployment

@flow(name="daily-crypto-collection")
async def daily_collection():
    config = WorkflowConfig({...})
    result = await archive_collection_flow(config)
    return result

deployment = Deployment.build_from_flow(
    flow=daily_collection,
    schedule="0 6 * * *"
)
```

### Custom Storage Integration
```python
from src.crypto_lakehouse.storage.base import BaseStorage

class CustomStorage(BaseStorage):
    def get_partition_path(self, **kwargs):
        # Custom path logic
        return custom_path

workflow.storage = CustomStorage(settings)
```

### Metrics Collection
```python
from src.crypto_lakehouse.core.metrics import MetricsCollector

metrics = MetricsCollector()
workflow = PrefectArchiveCollectionWorkflow(config, metrics)
result = await workflow.execute()
```

## ðŸ“ˆ Performance Optimization

### Throughput Optimization
- Increase `batch_size` to 100+ for better s5cmd utilization
- Use `max_parallel_downloads` up to 10 for high-bandwidth connections
- Enable `enable_batch_mode` for optimal s5cmd performance

### Memory Optimization
- Reduce `batch_size` for memory-constrained environments
- Use smaller `part_size_mb` values for limited memory
- Disable `download_checksum` for memory savings (not recommended)

### Network Optimization
- Enable `enable_resume` for unreliable connections
- Increase `timeout_seconds` for slow networks
- Use higher `retry_count` in s5cmd_extra_args

## ðŸš¨ Troubleshooting

### Common Issues

#### Matrix File Not Found
```
Error: Archive matrix file not found
Solution: Ensure enhanced_binance_archive_matrix.json exists
```

#### Invalid Configuration
```
Error: 'invalid_market' is not a valid market type
Solution: Use valid markets: spot, futures_um, futures_cm, options
```

#### Download Failures
```
Error: Download timeout or network error
Solution: Check network connectivity, increase timeout, enable resume
```

#### Permission Errors
```
Error: Permission denied writing to output directory
Solution: Ensure write permissions for output directory
```

### Performance Issues

#### Slow Downloads
- Check network bandwidth and latency
- Increase `max_parallel_downloads` (max 10 in production)
- Enable `enable_batch_mode` for better s5cmd performance
- Use SSD storage for better I/O performance

#### Memory Usage
- Monitor system memory during large collections
- Reduce `batch_size` if memory issues occur
- Use streaming processing for very large files
- Consider distributed processing for massive collections

## ðŸ“š Additional Resources

### Documentation Links
- **[Complete Guide](./enhanced-archive-collection.md)** - Full workflow documentation
- **[API Reference](./archive-collection-api.md)** - Developer integration guide
- **[Examples](./archive-collection-examples.md)** - Practical use cases and configurations
- **[Migration Guide](./enhanced-archive-collection.md#migration-guide)** - Upgrade from legacy workflows

### External Resources
- **Binance Archive**: https://github.com/binance/binance-public-data
- **s5cmd Documentation**: https://github.com/peak/s5cmd
- **Prefect Documentation**: https://docs.prefect.io/

### Support
- Review troubleshooting documentation
- Check performance optimization guides
- Submit detailed issue reports with configuration and error logs

---

**Ready for Production Use! ðŸš€**

The Enhanced Archive Collection Workflow provides enterprise-grade cryptocurrency data collection with comprehensive market coverage, optimized performance, and robust error handling.